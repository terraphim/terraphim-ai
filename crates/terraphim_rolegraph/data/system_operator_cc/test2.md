---
created: 2023-09-21T15:58:52 (UTC +01:00)
tags: []
source: https://www.sciencedirect.com/science/article/pii/S095183202300426X
author: Y.F. Niu
---

# Risk-informed operation and maintenance of complex lifeline systems using parallelized multi-agent deep Q-network - ScienceDirect

> ## Excerpt
> Lifeline systems such as transportation and water distribution networks may deteriorate with age, raising the risk of system failure or degradation. T…

---
## 1\. Introduction

Components in civil infrastructure systems, such as bridges, pipes, and electric wires, deteriorate over the life cycle of the systems due to corrosion of materials and other environmental factors. Component failures caused by the degradation may trigger a system-level failure event, e.g., [power outage](https://www.sciencedirect.com/topics/engineering/power-outage "Learn more about power outage from ScienceDirect's AI-generated Topic Pages"), service disruption, loss of network connectivity, or degenerate the systems’ the quality of service (QoS) like usability and [serviceability](https://www.sciencedirect.com/topics/engineering/serviceability "Learn more about serviceability from ScienceDirect's AI-generated Topic Pages") [\[1\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0001). Since civil infrastructure systems, especially lifeline systems including power, water, or gas transmission systems, have a great impact on [modern societies](https://www.sciencedirect.com/topics/social-sciences/modern-society "Learn more about modern societies from ScienceDirect's AI-generated Topic Pages"), not only can each failure cause huge socio-economic losses, but system-level malfunctions multiply the damage exponentially. To quantify the risk uncertainty of these system elements, numerous studies on risk modeling based on stochastic processes have been conducted \[[2](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0002),[3](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0003)\]. Based on these modeled risk, decision-makers should establish apposite risk-informed operation and maintenance (O&M) policies considering the inevitable changes in system-level QoS and the consequences of system failure \[[2](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0002),[4](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0004)\].

Various O&M strategies have been developed for structures in civil lifeline systems. For example, time-based maintenance (TBM) undertakes maintenance based on the predetermined repair time intervals, i.e., without monitoring or criteria evaluation \[[5](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0005),[6](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0006)\]. On the other hand, condition-based maintenance (CBM) aims to perform [preventive maintenance](https://www.sciencedirect.com/topics/engineering/preventive-maintenance "Learn more about preventive maintenance from ScienceDirect's AI-generated Topic Pages") based on the information acquired or interpreted in various forms \[[6](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0006),[7](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0007),[8](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0008),[9](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0009)\]. Although it is straightforward to scale and apply these O&M schemes to multiple components, the introduction of exogenous variables such as the cost of system failure events hampers their direct extensions to system-level decision-making processes. This is because component-level [optimality](https://www.sciencedirect.com/topics/engineering/optimality "Learn more about optimality from ScienceDirect's AI-generated Topic Pages") does not ensure the optimal decisions at the system level [\[10\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0010). Therefore, stakeholders and decision-makers of civil lifeline systems should be able to establish an O&M policy based on the system-level performance rather than on the conditions of individual components during the life cycle.

For O&M policy planning for a system consisting of components subject to uncertain failures, classical theories of system reliability \[[11](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0011),[12](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0012),[13](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0013)\] often introduce a Markovian model to handle the characteristics of the components and the system. If the same system control policy is applied over the life cycle, stationary conditions are achieved after a warm-up period. Thus, the system availability can be expressed as a closed-form expression under the assumption that components have constant mean failure rates [\[13\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0013). However, such an O&M policy based on the steady-state assumption is not flexible enough to cope with many plausible exceptions in practice, e.g., an old road facing its service life.

In efforts to obtain optimal O&M decisions without special assumptions such as steady-state conditions in a finite time horizon, a [Markov decision process](https://www.sciencedirect.com/topics/engineering/markov-decision-process "Learn more about Markov decision process from ScienceDirect's AI-generated Topic Pages") (MDP) has been considered as a preferred framework to formulate the stochastic processes of [structural deterioration](https://www.sciencedirect.com/topics/engineering/structural-deterioration "Learn more about structural deterioration from ScienceDirect's AI-generated Topic Pages") and the risk-informed O&M optimization. Because of the capability to quantify the expected cost or value of various maintenance actions in each state through [dynamic programming](https://www.sciencedirect.com/topics/social-sciences/dynamic-programming "Learn more about dynamic programming from ScienceDirect's AI-generated Topic Pages"), MDPs have been applied to establish the O&M policies for multi-state systems \[[14](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0014),[15](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0015)\] as well as component-level policies [\[16\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0016) for civil infrastructures. Although MDPs have a fundamental limit in direct applications to large systems due to the curse of dimensionality, i.e., the state and action spaces increase exponentially with the number of the components, researchers have proposed relaxing constraints or seeking approximate solutions \[[17](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0017),[18](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0018)\].

In particular, to overcome the challenges, recent studies have applied many deep reinforcement learning (DRL)-based algorithms \[[19](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0019),[20](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0020),[21](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0021),[22](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0022)\] to maintenance optimization problems in civil infrastructure systems \[[23](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0023),[24](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0024),[25](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0025)\]. More specifically, Yang et al. [\[26\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0026) utilized a deep Q-learning (DQN) framework for the optimal CBM planning of a multi-state system, and the framework has been further refined with the introduction of double DQN (DDQN) \[[27](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0027),[28](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0028),[29](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0029)\]. Moreover, multi-agent reinforcement learning (MARL) \[[30](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0030),[31](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0031),[32](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0032),[33](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0033)\], in which multiple agents learn policies to achieve the minimum costs using DRL, has been considered to improve the scalability of DRL applications for optimizing the maintenance of civil systems. For example, Andriotis and Papakonstantinou \[[10](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0010),[34](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0034)\] have developed centralized or decentralized MARL-based algorithms, respectively, in terms of sharing or not sharing parameters related to each agent's policy for large civil systems. Zhou et al. [\[35\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0035) also proposed MARL-based hierarchical algorithms to optimize the O&M decisions of large-scale multi-component systems. To enhance the performance of MARL, Nguyen et al. [\[36\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0036) introduced a value decomposition network (VDN) algorithm \[[32](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0032),[37](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0037)\], decomposing the centralized Q-value into the sum of decentralized Q-values defined for individual agents, to find the best O&M decisions. However, although these MARL algorithms successfully reduce the computational complexity, model training or hyperparameter tuning still requires considerable time even in the relaxed environment. In the numerical examples where the existing MARL methods are applied, the topologies are quite monotonous, such as series- or parallel-systems, and the number of components is also limited.

In this paper, we propose a parallelized multi-agent deep Q-network (PM-DQN), a divide-and-conquer algorithm that utilizes clustering-based multi-scale approaches \[[38](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0038),[39](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0039)\], for risk-informed decision optimization of multi-state flow systems by combining the system simplification with MARL based on parallel processing. In PM-DQN, the agents observe the states of multiple subsystems identified by clustering, and learn decentralized policies to minimize the factorized cost for subsystems. The algorithm guides the decentralized policies chosen by the agents toward the system-level optimality by repeating the learning process through parallel processing and tuning a key hyperparameter based on the results with less computational cost. As a result, the PM-DQN can efficiently manage large systems that are out of control in existing approaches.

This paper is organized as follows. [Section 2](https://www.sciencedirect.com/science/article/pii/S095183202300426X#sec0002) provides a description of the problem setting and the model assumptions for numerical examples, discusses the system-level sequential decision-making optimization problem, and introduces the DDQN and MARL frameworks. [Section 3](https://www.sciencedirect.com/science/article/pii/S095183202300426X#sec0010) proposes the PM-DQN algorithm based on the configuration of subsystems based on community detection and the cost factorization. The efficiency and superior performance of the proposed algorithm are demonstrated by numerical examples in [Section 4](https://www.sciencedirect.com/science/article/pii/S095183202300426X#sec0014), followed by a summary and concluding remarks in [Section 5](https://www.sciencedirect.com/science/article/pii/S095183202300426X#sec0017).

## 2\. Background information

### 2.1. Problem statement

#### 2.1.1. Descriptions of system state

In this study, we consider a multi-state flow system with components. It is assumed that the initial state of each component is intact, i.e., as-good-as-new (AGAN) state. The degradation level of components is identified as one of the following discretized damage states through annual observations: _AGAN, slight damage, moderate damage, extensive damage,_ and _collapse_. We assume that the system state is observed without errors. The [system degradation](https://www.sciencedirect.com/topics/engineering/degradation-system "Learn more about system degradation from ScienceDirect's AI-generated Topic Pages") state is given as a vector of component states, where is the state of component at time

As a performance indicator, the maximum flow capacity between two predetermined terminals is used to quantify the QoS of the system. It is assumed that the flow capacity of all components depends on each state as shown in [Table 1](https://www.sciencedirect.com/science/article/pii/S095183202300426X#tbl0001). Once the flow capacities of all components are determined, i.e., the system state is observed, the loss of the system QoS, can be calculated as the difference between the current maximum flow capacity and the maximum flow capacity under the initial system state through the max-flow algorithm \[[1](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0001),[40](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0040),[41](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0041)\]. Depending on the topology of the system, each component's flow capacity loss due to the deterioration has a different effect on the QoS of the system.

Table 1. Flow capacity of each component for each damage state.

| Damage state | AGAN | Slight | Moderate | Extensive | Collapse |
| --- | --- | --- | --- | --- | --- |
| Flow Capacity | 1.00 | 0.95 | 0.50 | 0.25 | 0 |

#### 2.1.2. Descriptions of maintenance and degradation process

Based on the system state information, the agent chooses one of the two options for O&M action for the component at time i.e., ‘Do Nothing (_DN_)’ and ‘Repair it to AGAN state (_R_).’ System-level action is defined as a vector of the actions taken for individual components. Although the size of the action space seems small at the component level, the number of system-level actions that a decision-maker considers each year is which increases exponentially with the number of components.

We assume that there is no limit to the number of components that can be repaired per step. When the decision-maker selects _R_ for one component, the state becomes the AGAN state deterministically, otherwise it remains the same or deteriorates with a certain probability called state-transition probability The stochastic [degradation process](https://www.sciencedirect.com/topics/engineering/degradation-process "Learn more about degradation process from ScienceDirect's AI-generated Topic Pages") of individual components is modeled as an independent discrete-time Markov process. That is, the state-transition probability for component depends only on the current state and action due to its [Markov properties](https://www.sciencedirect.com/topics/engineering/markov-property "Learn more about Markov properties from ScienceDirect's AI-generated Topic Pages"). While the state-transition probabilities for _R_ are the same for all components, the state-transition probabilities for _DN_ are modeled in two ways depending on the component types, each shown as matrices in the [Appendix A](https://www.sciencedirect.com/science/article/pii/S095183202300426X#sec0019). Once the system-level action for all components is determined in system state we can define the system-level state-transition probability based on the state-transition probabilities for all components.

#### 2.1.3. Descriptions of operation and maintenance costs

In the O&M process of civil lifeline systems, various variables (e.g., maintenance costs, system QoS, component malfunction) must be considered simultaneously at each time step. Because of trade-offs between life-cycle costs and the risk of systems, there is often no single solution that optimizes all these variables simultaneously. It is necessary to find an appropriate compromise among the conflicting goals. In multi-objective optimization, one can explore Pareto-optimal solutions that cannot improve an objective function without degrading other objective functions [\[42\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0042). However, to make a decision among Pareto-optimal solutions, the subjective preference of decision-makers must intervene inevitably.

One can handle this issue by transforming multiple objective functions into a single objective function through scalarization, i.e., the weighted sum formulation. In this approach, the optimal solution strongly depends on cost functions (or utility functions) used in the scalarization process. In this paper, the objective function which is the total cost at time is defined as the sum of the multiple objective functions scaled to cost as follows:(1)where is the maintenance cost for the component is the cost due to the shutdown of the component and is the system damage cost caused by the loss of system QoS. In [Eq. (1)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0001), conflicts with the other operation costs; and which depend on the state of components and system, decrease with a conservative O&M decision, while incurred by repairs increases. Therefore, a suitable compromise between these costs is desirable.

In numerical examples in [Section 4](https://www.sciencedirect.com/science/article/pii/S095183202300426X#sec0014), it is assumed that the maintenance cost of 1.0 is imposed when is _R_ regardless of and the shutdown cost of 1.0 is charged when component reached the collapse state. Loss of the system QoS, penalizes the system damage cost by a factor of as(2)where is a function representing the maximum flow capacity between predetermined two terminals under system state and is the initial system state vector, i.e., every component is in AGAN state.

If the term in [Eq. (1)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0001) is neglected, i.e., the objective function is given as a linear function of the component-level costs, the system-level optimization problem is decomposed into multiple component-level optimization problems. The optimal solutions to the [subproblems](https://www.sciencedirect.com/topics/engineering/subproblem "Learn more about subproblems from ScienceDirect's AI-generated Topic Pages") provide the system-level optimum. In most cases, however, given as a function of component state combinations is of great interest. Therefore, the objective function cannot be decomposed into component-level decentralized costs. In other words, the number of combinable policies to explore for achieving a system-level optimal solution grows exponentially with the system size.

### 2.2. System-level sequential maintenance optimization

In maintenance optimization problems for civil lifeline system, the agent's goal is to determine a policy as a mapping from the state to the action to minimize the Q-value, defined as the expected total discounted costs to the system's lifespan as(3)where is the discount factor to convert future costs into present values. Just as the component-level [optimality](https://www.sciencedirect.com/topics/engineering/optimality "Learn more about optimality from ScienceDirect's AI-generated Topic Pages") does not guarantee the system-level optimality, a greedy optimal decision at every time step cannot assure optimality over the long run in a finite-time horizon environment. This applies to sequential maintenance optimization problems for civil lifeline system O&M throughout the life cycle. An O&M policy is defined to be better than or equal to another O&M policy if and only if its expected cost is smaller than or equal to that of for all states [\[43\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0043). That is, the optimal O&M policy leads the Q-value to its minimum.

In value iteration \[[43](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0043),[44](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0044)\], considering this point, the Q-value is updated by continuously choosing the action with the lowest Q-value as(4)

After initializing all the Q-value estimates to zero, the values are updated iteratively according to the following Bellman equation:(5)where is the Q-value on state and action estimated at the iteration. The value [iteration algorithm](https://www.sciencedirect.com/topics/engineering/iteration-algorithm "Learn more about iteration algorithm from ScienceDirect's AI-generated Topic Pages") works well in simple environments but does not apply to environments with large state and action spaces. In particular, evaluating the Q-value of every combinable state-action pair in complex environments is nearly impossible.

### 2.3. Double Deep Q-Network with prioritized experience replay

To overcome the computational limitation, Watkins and Dayan [\[22\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0022) proposed to apply a model-free reinforcement learning algorithm called Q-learning. In the Q-learning algorithm, instead of [Eq. (5)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0005), the following equation is used to update Q-values iteratively:(6)where is the learning rate; and is the target value.

The \-greedy algorithm is often implemented to expedite the exploration and exploitation process for Q-learning [\[43\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0043). In detail, after initializing Q-value to zero, an agent selects a random action (i.e., exploration) with probability or the action minimizing Q-value (i.e., exploitation) with probability It is common to start with the value of close to 1 and gradually decrease it as information about the environment accumulates.

In a powerful deep reinforcement learning (DRL) framework developed by Mnih et al. \[[19](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0019),[20](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0020)\], a parameterized deep Q-network (DQN) is introduced to approximate the Q-value. Each layer in the DQN outputs a [linear combination](https://www.sciencedirect.com/topics/engineering/linear-combination "Learn more about linear combination from ScienceDirect's AI-generated Topic Pages") of inputs and parameters. The framework introduces [activation functions](https://www.sciencedirect.com/topics/engineering/activation-function "Learn more about activation functions from ScienceDirect's AI-generated Topic Pages") between layers, such as rectified linear unit (ReLU), exponential linear unit (ELU), and scaled ELU (SELU), to describe the complex relationships beyond linearity. However, DQN tends to underestimate the Q-values in stochastic environments due to the ‘min’ operator in calculating the target value Double DQN (DDQN) [\[27\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0027) performs the action selection and Q-value evaluation separately using two different DQNs (i.e., online and target networks), thereby avoiding the underestimation of the Q-values and stabilizing the learning process. The target value is rewritten as(7)where is the Q-value estimated using the online network with parameters and is the Q-value estimated using the target network with parameters respectively. The parameters are periodically updated to the parameters every steps.

In addition, one can introduce the experience replay [\[45\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0045), in which agents store the experiences as tuples in a replay buffer and update the online network parameters based on a batch of uniformly sampled tuples from the replay buffer. The uniformly drawn samples significantly reduce the variance of updates owing to the weakened correlation, thereby suppressing the oscillation or divergence of the parameters during the training process. Combining these techniques, DDQN calculates the loss function and updates the online network parameters to minimize by the [gradient descent](https://www.sciencedirect.com/topics/engineering/gradient-descent "Learn more about gradient descent from ScienceDirect's AI-generated Topic Pages") as follows:(8)(9)where represents a uniform distribution over the replay buffer and is a batch of the tuples sampled from

Agents can gauge the importance of each experience by the temporal difference error (TD-error), i.e., the difference between the expected Q-values before and after the experience. In prioritized experience replay (PER) \[[46](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0046)\], the alternative sampling density of a batch of experience tuples which was uniformly distributed, is given as:(10)where is the TD-error; and is an importance sampling hyperparameter, with corresponding to the existing experience replay. The loss function in [Eq. (8)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0008) can be calculated more efficiently using importance sampling as follows:(11)where denotes the probability mass function of the discrete uniform distribution; and is the likelihood ratio to compensate for the bias caused by introducing the alternative sampling density The initial learning process is highly non-stationary because the online and target networks have not been trained enough. Therefore, even if the estimate is slightly biased, stabilization should be the top priority at the beginning phase of learning, and importance sampling correction may be considered later. To this end, in [Eq. (11)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0011) is replaced by , defined as(12)where is the number of experiences in the replay buffer and is a hyperparameter controlling how much to compensate for the bias. As is gradually annealed toward 1.0, starting from a low value (e.g., typically around 0.4 to 0.6), the bias is completely compensated. When combined, DDQN and PER exert a substantial [synergistic effect](https://www.sciencedirect.com/topics/engineering/synergistic-effect "Learn more about synergistic effect from ScienceDirect's AI-generated Topic Pages"), thereby neutralizing a significant portion of the existing limitations of Q-learning algorithms.

### 2.4. Factorization in multi-agent reinforcement learning

In addition to increasing the efficiency of DRL like DDQN with PER, dividing the original problem into multiple subproblems can be a more effective solution to overcome the curse of dimensionality caused by the exponentially increasing number of states and actions. A divide-and-conquer strategy called a multi-agent reinforcement learning (MARL) deploys multiple agents in each segmented state and action spaces, and the agents learn policies to achieve the minimum costs independently or through cooperation.

If there are no cost functions that agents jointly contribute, a given environment can be simplified by decomposing it into several smaller independent environments \[[30](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0030),[47](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0047)\]. In other words, the optimal decision set in sub-problems yields the same solution as the optimal global decision-making. Then, the size of the action space is shrunk from to where is the number of agents, and is redefined as the number of available actions for the agent. As a result, one can handle environments with large spaces of states and actions based on the large scalability.

However, in most of complex environments, cost functions take a combination of various states and actions as input, e.g., [Eq. (1)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0001). This leads to the multi-agent credit allocation problem [\[33\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0033), in which the contributions of individual agents to the cost function must be accurately inferred. To this end, the problem can be segmented into multiple independent environments by factorizing the centralized cost into the sum of decentralized costs through [neural networks](https://www.sciencedirect.com/topics/social-sciences/neural-network "Learn more about neural networks from ScienceDirect's AI-generated Topic Pages") called value decomposition networks (VDNs) [\[32\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0032) or user-defined functions. No matter how sophisticated the original cost is segmented, accuracy loss is inevitable during the [factorization process](https://www.sciencedirect.com/topics/engineering/factorization-process "Learn more about factorization process from ScienceDirect's AI-generated Topic Pages"), thereby resulting in a trade-off between accuracy and scalability.

## 3\. Proposed algorithm: parallelized multi-agent deep Q-network (PM-DQN)

The MARL method combined with DDQN and PER improves the scalability with minimal loss of accuracy, allowing the MDP framework to be applied to complex environments. However, as the number of components in the system increases, the accuracy loss of the MARL method gradually accumulates, eventually leading to a failure in [optimal policy](https://www.sciencedirect.com/topics/engineering/optimal-policy "Learn more about optimal policy from ScienceDirect's AI-generated Topic Pages") search. To alleviate the scalability issue, in this section, we propose a novel MARL algorithm termed “Parallelized Multi-agent Deep Q-Network” (PM-DQN), which assigns multiple agents to subsystems based on community detection, and makes the agents explore decentralized cost-minimizing policies under various hyperparameter values in multiple processing units. The proposed algorithm consists of three steps differentiated from others as follows: (1) system simplification based on network clustering for effective agent allocation, (2) factorization of centralized costs based on a predefined function, and (3) the hyperparameter tuning with parallel processing. [Fig. 2](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0002)(a) and (b) respectively illustrate the proposed algorithm's microscopic and macroscopic structures of operations.

![Fig 2](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr2.jpg)

1.  [Download : Download high-res image (948KB)](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr2_lrg.jpg "Download high-res image (948KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr2.jpg "Download full-size image")

Fig. 2. (a) Learning process using DDQN with PER in a processing unit, and (b) overall procedure of Parallelized Multi-agent Deep Q-Network (PM-DQN) algorithm.

### 3.1. Step 1: identification of subsystems based on community detection

In optimizing MARL-based O&M strategies in large-scale systems, it is necessary to find an appropriate balance between minimizing accuracy loss and efficient reduction of the state and action spaces. To this end, we propose a decomposition of the system into a few subsystems, i.e., communities of components. By grouping deeply related components in terms of functionality or densely connected components into one subsystem, the existing system can be simplified into another system of subsystems. This can also lower computational complexity that may scale exponentially with the number of components, while minimizing loss of information. Various algorithms such as spectral [clustering algorithms](https://www.sciencedirect.com/topics/engineering/clustering-algorithm "Learn more about clustering algorithms from ScienceDirect's AI-generated Topic Pages") \[[38](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0038),[48](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0048)\] and Markov clustering algorithm \[[49](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0049),[50](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0050)\] have been proposed for community detection. In this paper, the target system is represented by subsystems instead of the existing components based on the Girvan-Newman algorithm \[[39](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0039),[51](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0051)\].

The Girvan-Newman algorithm sequentially removes edges with the highest edge-betweenness, which means the probability that the shortest paths between all node pairs in the graph go through the edge [\[52\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0052). As edges with high edge-betweenness are removed, the entire graph representing the system is divided into several isolated clusters, and the process of edge removal ends when all edges are removed. In Girvan-Newman algorithm, the modularity is introduced to stop the process at an appropriate time [\[51\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0051). The modularity represents the difference between the actual number of intra-cluster edges in the existing graph and the expected number of intra-cluster edges when reconstructing the graph while preserving the degree of each node, and is defined as(13)where is the number of isolated clusters; is the number of edges in the graph; is the number of intra-cluster edges in cluster and is the sum of degrees of the nodes in cluster Modularity is updated whenever an edge is removed, and when it is maximized, one can identify subsystems that are separated from each other.

Once subsystems in the target system are identified using the Girvan-Newman algorithm, we can assign an agent to learn the O&M strategy of each subsystem. However, there is a problem in utilizing the community detection result for PM-DQN; since all agents must be trained for optimal decision-making for the system-level optimal policy, the convergence of PM-DQN is governed by the slowest learning rate among agents (generally the agent with the largest state and action spaces). If components are concentrated in specific clusters, the learning time of the agents increases exponentially, thereby leading to a decrease in learning efficiency of PM-DQN. Therefore, a procedure for adjusting the subsystems to an even size is necessary. To this end, after initial grouping based on the Girvan-Newman algorithm, some components in the largest subsystem are reallocated to other neighboring subsystems. In this process, the number of edges connecting subsystems should be minimized to prevent loss of calculation accuracy due to system simplification.

[Fig. 3](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0003) shows a simplified California gas distribution system (modified from [\[53\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0053)) with 48 gas [substations](https://www.sciencedirect.com/topics/engineering/substations "Learn more about substations from ScienceDirect's AI-generated Topic Pages") of two types and 60 bidirectional pipelines. It is assumed that the pipelines are intact during the lifetime of the system, and only the deterioration of the gas substations, whose states can be periodically identified, is considered. The system can be described by a graph model with 48 nodes and 60 edges. [Table 2](https://www.sciencedirect.com/science/article/pii/S095183202300426X#tbl0002) shows how modularity and the number of detected clusters change with edge removal by the Girvan-Newman algorithm. As the edges are removed continuously, the number of clusters increases, but the modularity peaks at 8 clusters. However, the detected clusters consist of five to seven components as shown in [Fig. 4](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0004)(a), resulting in differences in the action space of the agents. Since the learning rate of the agent managing the largest subsystem is lower than that of the other agents, the benefits of parallel processing in simultaneous training of all agents can be underutilized. To compensate for this limitation, one component is reallocated from the largest subsystem to the neighboring smallest subsystem 3 as shown in [Fig. 4](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0004)(b), making the number of components within subsystems as uniform as possible. In contrast to the previous subsystems in [Fig. 4](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0004)(a), all subsystems in [Fig. 4](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0004)(b) consist of the same number of components, so the learning rate of the agent is expected to be similar. The needs for this procedure are discussed through numerical examples in [Section 4](https://www.sciencedirect.com/science/article/pii/S095183202300426X#sec0014) by comparing the learning efficiency with or without the reallocation.

![Fig 3](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr3.jpg)

1.  [Download : Download high-res image (157KB)](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr3_lrg.jpg "Download high-res image (157KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr3.jpg "Download full-size image")

Fig. 3. California gas distribution system with 48 multi-state gas substations.

Table 2. Modularity and number of clusters according to edge elimination.

| No. of eliminated edge | 0 | 1 | 3 | 7 | 11 | 13 | 15 | 18 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **Modularity** | 0 | 0.443 | 0.565 | 0.685 | **0.689** | 0.673 | 0.647 | 0.608 |
| **No. of clusters** | 1 | 2 | 4 | 6 | **8** | 10 | 12 | 14 |

![Fig 4](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr4.jpg)

1.  [Download : Download high-res image (255KB)](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr4_lrg.jpg "Download high-res image (255KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr4.jpg "Download full-size image")

Fig. 4. (a) Subsystems detected by Girvan-Newman algorithm, and (b) subsystems with uniformly reallocated components.

### 3.2. Step 2: factorization of centralized costs

After assigning one agent to each subsystem detected in Step 1, multiple agents make O&M decisions by comprehensively considering the state of components within each subsystem. To carry out the decision making independently but efficiently for all subsystems, the total cost should be factorized into individual subsystems based on effective multi-agent credit allocation. For example, the total cost in [Eq. (1)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0001) is divided into two parts: decentralized costs (e.g., and and centralized costs (e.g., Since decentralized costs obviously arise from each component, it makes sense to impose the sum of decentralized costs of the components in a subsystem to the corresponding agent. However, allocating the centralized cost to individual agents requires the introduction of VDN or predefined functions. To this end, PM-DQN finds the subsystems that are presumed to cause the loss of system, and selectively allocates centralized costs to those subsystems based on a predefined function. More specifically, by splitting the centralized cost and summing up the decentralized costs, the total decentralized cost for subsystem is expressed as(14)where is redefined as the action chosen by agent at time is the factorized cost transferred from the centralized cost to subsystem and is the hyperparameter that determines the weight of the factorized cost. The factorized cost is predefined as follows:(15)where is the QoS loss in the subsystem; and is a subset of which is the state vector of components belonging to the subsystem. While is calculated based on the maximum flow capacity between two predetermined terminals, is defined as the difference between the total inflow and outflow of the subsystem. The structure of [Eq. (15)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0015) is similar to that of [Eq. (2)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0002) before factorization, but individual agents infer the causality between QoS losses of system and their subsystems using the min operator.

Using [Eqs. (6)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0006), [(7)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0007), [(14)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0014), and [(15)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0015), one can calculate individual Q-values for each agent In this paper, we utilize the DDQN combined with PER (introduced in [Section 2.3](https://www.sciencedirect.com/science/article/pii/S095183202300426X#sec0008)) for more efficient and stable Q-learning. More specifically, the online network with parameters and the target network with parameters for take the system state vector and the current time step as inputs, and respectively output the Q-values and according to each action in the form of vectors. Then, the optimal action based on the estimated is converted into an \-dimensional one-hot encoding vector, which is multiplied with the vector form of to update the online Q-value. Unlike these online parameters that are updated every step, the target network parameters are periodically updated to the online parameters every steps. It should be noted that, unlike VDN, the expected total life-cycle cost of the system is not equal to the sum of those caused by the factorized costs [\[37\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0037). As all agents choose the actions that minimize their respective the system-level action set is redefined as(16)

In the learning process of DDQN, the performance of the PM-DQN depends significantly on the hyperparameter in [Eq. (14)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0014). Therefore, the hyperparameter value should be appropriately determined depending on the environment (e.g., system topologies, O&M costs, discount factor In addition, there is no way to find the optimal value analytically, while heuristics require substantial computational costs for

### 3.3. Step 3: parallel processing-based hyperparameter tuning

For effective hyperparameter tuning, we introduce parallel processing in the proposed algorithm. Processing units (e.g., CPUs or GPUs) are classified into five groups with different hyperparameter values. In each unit, agents explore the optimal decentralized policies using the \-greedy algorithm simultaneously under each given hyperparameter as illustrated in [Fig. 2](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0002)(a). After training agents for a sufficiently large number of epochs, called a cycle, one can judge the superiority of policies based on hyperparameter values through the expected total life-cycle costs Prior to the next cycle, the parameters of the online and target networks in all processing units, and for are synchronized with those showing the best performance in the previous cycle, as shown in [Fig. 2](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0002)(b), thereby contributing to improving the master policy. Through the synchronization process, some rarely explored near-optimal policies can be propagated to other processing units, resulting in significant performance gains. Then, hyperparameter is tuned as follows:(17)where is an exponentially decaying step size; and In addition, when the expected life-cycle cost has sufficiently converged through comparison of results of the parallel processing, the algorithm is terminated early. [Algorithm 1](https://www.sciencedirect.com/science/article/pii/S095183202300426X#tbl0010) provides a pseudo-code of the proposed PM-DQN algorithm.

Algorithm 1. Parallelized Multi-agent Deep Q-Network (PM-DQN).

| Identify subsystems through community detection and assign an agent to each subsystem |
| --- |
| Initialize replay buffer for |
| Initialize the online network with random parameters for |
| Initialize the target network with random parameters for |
| Initialize hyperparameters |
| **for** **to** **do** |
|  **for** **to** **do** |
|   **for** **to** **do** |
|    Initialize state where |
|    **for** **to** **do** |
|     **for** **to** **do** |
|       |
|      Observe for |
|     **end** |
|     Observe system damage cost |
|     **for** **to** **do** |
|      Calculate the factorized cost for subsystem |
|      Store experience in with TD-error |
|      Sample minibatch of experiences with probability from |
|      Compute importance-sampling weight |
|      Compute target value |
|      Update |
|      Perform gradient descent on w.r.t. |
|      Update every steps |
|     **end** |
|    **end** |
|   **end** |
|  **end** |
|  Synchronize the network parameters |
|  Tune hyperparameters for k∈\[−2,2\] |
| **end** |

## 4\. Numerical examples

To demonstrate the proposed PM-DQN, we consider two numerical examples inspired by Andriotis & Papakonstantinou [\[10\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0010) and Stern et al. [\[53\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0053): (1) a multi-state lifeline network system with 15 components and (2) the simplified California gas distribution system abovementioned. The life-cycle span of each system TLS is set to 50 steps (i.e., 50 years) with a discount factor of γ\=0.95. The coefficient fSD in [Eqs. (2)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0002) and [(15)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0015) for the system damage cost cSD is set to 5.0 in both examples.

To build and implement our own custom environments, we use OpenAI Gym [\[54\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0054), a toolkit for reinforcement learning research. All experiments are performed using the Keras [deep learning](https://www.sciencedirect.com/topics/engineering/deep-learning "Learn more about deep learning from ScienceDirect's AI-generated Topic Pages") Python library [\[55\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0055) with Tensorflow backend [\[56\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0056) on a server with 2 Intel(R) Xeon(R) CPU Gold 6240R CPUs at 2.40 GHz and 256GB [RAM](https://www.sciencedirect.com/topics/engineering/reliability-availability-and-maintainability-reliability-engineering "Learn more about RAM from ScienceDirect's AI-generated Topic Pages"). The online and target networks consist of three fully connected hidden layers, each with SELU [activation functions](https://www.sciencedirect.com/topics/engineering/activation-function "Learn more about activation functions from ScienceDirect's AI-generated Topic Pages") per example. For stochastic [gradient descent](https://www.sciencedirect.com/topics/engineering/gradient-descent "Learn more about gradient descent from ScienceDirect's AI-generated Topic Pages") on the network parameter space, we use the Nesterov-accelerated Adaptive Moment Estimation (Nadam) optimizer [\[57\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0057), combining the Adaptive Movement Estimation (Adam) with Nesterov momentum [\[58\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0058). DDQNs explore and exploit complex environments using the ε\-greedy algorithm mentioned in [Section 2.3](https://www.sciencedirect.com/science/article/pii/S095183202300426X#sec0008), where the exploration probability ε decreases from 0.5 to 0 for every cycle along with the [cosine function](https://www.sciencedirect.com/topics/engineering/cosine-function "Learn more about cosine function from ScienceDirect's AI-generated Topic Pages") [\[59\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0059). For model training based on parallel processing on multiple processors, the multiprocessing library [\[60\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0060) is implemented. [Appendix B](https://www.sciencedirect.com/science/article/pii/S095183202300426X#sec0020) provides more detailed information about hyperparameters for PM-DQN.

Since it is intractable to obtain the [optimal policy](https://www.sciencedirect.com/topics/engineering/optimal-policy "Learn more about optimal policy from ScienceDirect's AI-generated Topic Pages") for these examples due to the curse of dimensionality, two conventional O&M schemes and two MARL methods are also implemented as baseline policies as follows to confirm the superior performance of the PM-DQN:

-   •
    
    Condition-based maintenance (CBM): agents repair components that have deteriorated below optimized threshold states. Since the number of all combinable threshold cases is exponentially proportional to the number of components n, it is extremely time-consuming to evaluate the system life-time cost for all policies. In numerical examples, the optimal threshold state set is explored through iterations that sequentially update the optimal threshold state of each component that minimizes the system life-cycle cost. The number of threshold combinations per iteration scales linearly with the number of components.
    
-   •
    
    Time-based maintenance (TBM): agents periodically repair individual components, regardless of their state, at certain time intervals optimized for each component. To find the optimal repair intervals, we use iterations in the same form as CBM's policy exploration.
    
-   •
    
    Subsystem-level optimal maintenance (SOM): agents assigned to subsystems independently learn policies to minimize the cost of each subsystem in [Eq. (14)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0014), where the factorized centralized cost cf,j(St) depends only on the QoS loss in the jth subsystem, cf,jSOM(Stj)\=fSD·Lj(Stj), instead of [Eq. (15)](https://www.sciencedirect.com/science/article/pii/S095183202300426X#eqn0015). Other than the factorized cost, the identified subsystems to which agents are assigned and the hyperparameters in [Appendix B](https://www.sciencedirect.com/science/article/pii/S095183202300426X#sec0020) are shared with PM-DQN.
    
-   •
    
    Deep Centralized Multi-agent Actor Critic (DCMAC) [\[10\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0010): agents learn policies based on two separate [neural networks](https://www.sciencedirect.com/topics/social-sciences/neural-network "Learn more about neural networks from ScienceDirect's AI-generated Topic Pages"), called actor-critic methods, each approximating a centralized policy function and the expected total life-cycle costs. Actions on individual components are learned conditionally independent of each other. The details of structures and hyperparameters tuned for DCMAC in each numerical examples are given in [Appendix B](https://www.sciencedirect.com/science/article/pii/S095183202300426X#sec0020). In Example 2, to shorten the training time and streamline the process of efficiently exploring learning rates for actor and critic networks, parallelized DCMAC is introduced with periodic synchronization and compared to single-processing DCMAC.
    

### 4.1. Example 1: multi-state general system with 15 components

A 15-component system is represented as a _general_ system, i.e., not series- or parallel-system, in [Fig. 5](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0005). All components are assumed to be Type I. The system QoS is defined as the maximum flow capacity MFsys between both left and right sides, which is 2.0 when all components operate in the AGAN state. As indicated by the hatched blocks in [Fig. 5](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0005), three subsystems of five components are detected based on the Girvan-Newman algorithm. By grouping components, the size of the action space is shrunk from 215\=32,768 to 3×25\=96, since the three agents only share information about the system state St and select actions atj for j∈\[1,3\] independently.

![Fig 5](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr5.jpg)

1.  [Download : Download high-res image (249KB)](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr5_lrg.jpg "Download high-res image (249KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr5.jpg "Download full-size image")

Fig. 5. A general system with 15 multi-state components, simplified by three subsystems.

Before comparing the results of the proposed algorithm with those of the four baseline policies, we discuss the appropriateness of periodic synchronization in the PM-DQN. [Table 3](https://www.sciencedirect.com/science/article/pii/S095183202300426X#tbl0003) shows the average life-cycle costs and 95% confidence intervals of the realization of the policies trained on parallel units with and without periodic synchronization in the PM-DQN. The mean converges rapidly to the optimal life-cycle cost when accompanied by periodic synchronization. The standard deviation also decreases significantly, and the [coefficient of variation](https://www.sciencedirect.com/topics/engineering/coefficient-of-variation "Learn more about coefficient of variation from ScienceDirect's AI-generated Topic Pages") (c.o.v.) is less than 1% after the fourth cycle (i.e., 4,000 epochs), at which hyperparameter ω is tuned around the optimal value of 4.75. Since the step size λ\=0.25 is already small enough, the expected life-cycle cost of the policy explored at this time does not have a large difference from the optimal life-cycle cost after the 10th cycle. That is, when the step size and c.o.v. are sufficiently small (e.g., less than 0.5 and 1%, respectively), the algorithm can be terminated early without significant loss in terms of performance compared to when it progressed to the end. On the other hand, with single-cycle learning, although learning through the same number of epochs, the life-cycle cost estimates have a significant variance, and the mean cost is also higher than that of periodic synchronization. This inferior performance arises because there is no chance to tune the hyperparameter ω and explore good policies in many ways. Some outliers inevitably occur during parallel processing, but there are no other means for correcting them. Conversely, even if one or two learners find a rare near-optimal policy, these policies are not propagated to most computational resources and are not further developed.

Table 3. Expected life-cycle costs and 95% confidence intervals of realization of the trained policies with and without periodic synchronization in PM-DQN.

| Number of Epochs | w/ Periodic Sync | w/o Periodic Sync |
| --- | --- | --- |
| 1,000 | 46.04±35.99 |  |
| 2,000 | 33.65±3.44 |  |
| 4,000 | 29.05±0.48 |  |
| 6,000 | 28.71±0.53 |  |
| 10,000 | 28.33±0.54 | 40.99±20.20 |

In this example, 5×15\=75 and 50×15\=750 combinations of threshold state and time interval sets are explored to optimize CBM and TBM, respectively, through five iterations. As a result, the optimal threshold state set for CBM and the repair interval set for TBM are determined, as shown in [Appendix C](https://www.sciencedirect.com/science/article/pii/S095183202300426X#sec0021). In contrast, since MARL methods including SOM, PM-DQN, and DCMAC take the state combination of the components in a subsystem or a system or centralized costs as input, it is difficult to specify the damage state at which agents repair individual components. The performances of baselines and the proposed policy are estimated through 1,000 demonstrations in terms of the total life-cycle costs ctot, and the results are summarized in [Table 4](https://www.sciencedirect.com/science/article/pii/S095183202300426X#tbl0004) with the time required for iterative operation or model training. The TBM policy was evaluated as the most ineffective one due to the stochastic environment. Although the SOM shares the subsystems and hyperparameters for model learning with PM-DQN, each agent learns the subsystem-level optimal policy, resulting in higher life-cycle costs than PM-DQN at the system level. On the other hand, the optimal policy learned by the PM-DQN shows the lowest estimated life-cycle cost, which is almost identical to those of the optimal policies proposed by CBM and DCMAC. Comparing the computational time among them, the advantage of PM-DQN in computational efficiency becomes clear. CBM takes 3.5 times longer than the computational time of PM-DQN, even though search of all combinations is replaced with iterations. Although DCMAC requires about 47.5x more learning time than PM-DQN due to the difference in the number of processors, it can be shortened through parallelization and the improved convergence speed is compared on the complex system in Example 2.

Table 4. Life-cycle costs and computational time of PM-DQN and four baselines for Example 1.

| Method | Total life-cycle cost | Time (_sec_) |
| --- | --- | --- |
| CBM | 28.42 | 9,323 |
| TBM | 56.71 | 99,174 |
| SOM | 35.41 | 2,523 |
| DCMAC (single processing) | 28.57 | 125,773 |
| **PM-DQN** | **28.33** | **2,647** |

[Fig. 6](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0006)(a) and (b) show the realization of the O&M process for the individual subsystems according to the optimal policies based on SOM and PM-DQN, respectively. In these figures showing time histories of component-level costs in each subsystem, blue bars indicate maintenance costs cM,i spent on repairs, while red bars, representing shutdown costs cS,i, are not marked on the chart because all components are repaired before shutdown. Under the SOM policy, each agent immediately takes the O&M policy to minimize the loss of QoS in individual subsystems, thereby repairing subsystems immediately even minor damage at the subsystem level as if they were part of a series system as shown in [Fig. 6](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0006)(a). In the PM-DQN policy, the agent managing subsystem 3 operates in the same way as in the SOM policy, because subsystem 3 significantly influences the system QoS. In contrast, the other two subsystems connected in parallel are treated even more tolerantly than the SOM, since the two act as a detour to each other and the failure of either subsystem does not directly lead to the loss of system QoS. This difference between these two algorithms is clearly shown in [Fig. 7](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0007), which illustrates the annual average flow capacity loss in individual subsystems and the system under all policies. As can be seen from the results of all policies, the QoS loss in subsystem 3 plays a dominant role in the QoS loss of the system due to its series-connected topology in the system.

![Fig 6](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr6.jpg)

1.  [Download : Download high-res image (1MB)](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr6_lrg.jpg "Download high-res image (1MB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr6.jpg "Download full-size image")

Fig. 6. Life-cycle O&M results during an epoch in Example 1: component-level costs (i.e., cM,i and cS,i) and loss of QoS in each subsystem (i.e., cf,jSOM\=fSD·Lj) under (a) the SOM policy, and (b) the PM-DQN policy.

![Fig 7](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr7.jpg)

1.  [Download : Download high-res image (129KB)](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr7_lrg.jpg "Download high-res image (129KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr7.jpg "Download full-size image")

Fig. 7. Annual average loss of QoS in individual subsystems or system in Example 1 under each policy.

### 4.2. Example 2: the simplified California gas distribution system with 48 gas substations

Example 2 deals with a more realistic lifeline system, the California gas distribution system in [Fig. 3](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0003). Unlike Example 1, there are two types of components in the system; some components located in the west, close to the Pacific Ocean, are modeled as type II components with a high deterioration rate, while the rest are modeled as Type I. The maximum flow capacity MFsys between two [terminal nodes](https://www.sciencedirect.com/topics/engineering/terminal-node "Learn more about terminal nodes from ScienceDirect's AI-generated Topic Pages") is given as 1.0 when all [substations](https://www.sciencedirect.com/topics/engineering/substations "Learn more about substations from ScienceDirect's AI-generated Topic Pages") operate in the AGAN state. Through community detection shown in [Fig. 4](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0004)(a), the size of the action space is shrunk from 248≅2.81×1014 to 25+6×26+27\=544. After resizing the subsystems as shown in [Fig. 4](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0004)(b), the largest action space is reduced from 27\=128 to 26\=64.

Before comparing the performance of the proposed algorithm with the baselines, we examine the needs for component reallocation discussed in [Section 3.1](https://www.sciencedirect.com/science/article/pii/S095183202300426X#sec0011). Boxplots in [Fig. 8](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0008) show the expected life-cycle costs before and after reallocating the subsystem sizes evenly for learning cycles of PM-DQN. Outliers under the worst performing hyperparameters are excluded for visualization purposes. Comparing the training results up to the third cycle, in which sufficient learning has not been achieved yet, the variance of the learning rate of PM-DQN without reallocation is significantly larger than that with reallocation. This is because the policy search for a relatively large number of state and action spaces has not been conducted effectively and sufficiently in the largest subsystem 2. While their variances become similar after the 4th cycle, the former's estimated life-cycle cost becomes equal to that of the latter only after the 10th cycle is completed. As learning continues, both expected costs keep decreasing, achieving almost the same life-cycle cost. This shows that PM-DQN works well for large systems with components that have different [degradation processes](https://www.sciencedirect.com/topics/engineering/degradation-process "Learn more about degradation processes from ScienceDirect's AI-generated Topic Pages"), although the convergence rate becomes lower than before.

![Fig 8](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr8.jpg)

1.  [Download : Download high-res image (343KB)](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr8_lrg.jpg "Download high-res image (343KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr8.jpg "Download full-size image")

Fig. 8. Expected life-cycle costs for learning cycles with and without reallocation in PM-DQN.

The performances of baselines and MARL-based policies including the PM-DQN without reallocation estimated through 1,000 demonstrations are summarized in [Table 5](https://www.sciencedirect.com/science/article/pii/S095183202300426X#tbl0005). The optimal CBM and TBM policies are explored through five iterations as in Example 1. As a result, the threshold state set and repair time interval set are respectively determined, as shown [Appendix C](https://www.sciencedirect.com/science/article/pii/S095183202300426X#sec0021). As in Example 1, the TBM policy shows inferior results due to the stochastic environment, and the life-cycle cost of the CBM policy, showing excellent performance in Example 1, is higher than those of other MARL-based algorithms. While CBM evaluates the effect of individual components on system QoS at the component level and determines the state thresholds for repair, the system QoS fundamentally depends on the combination of states of various components. This characteristic becomes more prominent as systems get more complex, which is confirmed through the performance change of CBM compared to MARL-based approaches in Example 1 and Example 2. Although the CBM policy ranks with the DCMAC and PM-DQN policies in a simple system in Example 1, it faces the limitations in Example 2.

Table 5. Life-cycle costs and computational time of PM-DQN and four baselines for Example 2.

| Method | Total life-cycle cost | Time (_sec_) |
| --- | --- | --- |
| CBM | 89.95 | 20,126 |
| TBM | 161.61 | 214,140 |
| SOM | 88.82 | 103,456 |
| DCMAC (parallel processing) | 87.13 | 137,211 |
| PM-DQN (w/o Reallocation) | 86.77 | 97,437 |
| **PM-DQN (w/ Reallocation)** | **86.60** | **101,934** |

This also applies to SOM. Although the SOM policy may be an optimal decision at each subsystem level, the result of the optimal PM-DQN policy in [Table 5](https://www.sciencedirect.com/science/article/pii/S095183202300426X#tbl0005) shows that the subsystem-level optimal policy does not match that at the system level, where the hyperparameter ω is eventually tuned to 23.90. [Fig. 9](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0009) shows a simplified California gas system consisting of 9 subsystems, and the realizations of the system O&M by SOM and PM-DQN policies are shown in [Fig. 10](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0010)(a) and (b), respectively. To minimize the QoS loss at the subsystem level, SOM manages subsystems 3 and 4 conservatively compared to PM-DQN even though they are connected in parallel. Accordingly, the loss of QoS in individual subsystems is well maintained at a low level in the SOM policy as shown in [Fig. 10](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0010)(a). However, because large maintenance cost is required for the SOM policy, the life-cycle cost under the policy shows inferior results to the PM-DQN and DCMAC policies.

![Fig 9](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr9.jpg)

1.  [Download : Download high-res image (93KB)](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr9_lrg.jpg "Download high-res image (93KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr9.jpg "Download full-size image")

Fig. 9. Simplified California gas distribution system consisting of subsystems.

![Fig 10](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr10.jpg)

1.  [Download : Download high-res image (1MB)](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr10_lrg.jpg "Download high-res image (1MB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr10.jpg "Download full-size image")

Fig. 10. Life-cycle O&M results during an epoch in Example 2: component-level costs (i.e., cM,i and cS,i) and loss of QoS in each subsystem (i.e., cf,jSOM\=fSD·Lj) under (a) the SOM policy, and (b) the PM-DQN policy.

In contrast, the PM-DQN policy considers the QoS loss of subsystems and systems simultaneously. Subsystems except for subsystems 3 and 4 have no separate bypass. This means that, even if agents maintain subsystems 3 and 4 loosely, other subsystems should be managed conservatively as shown in [Fig. 10](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0010)(b). In this way, system simplification based on community detection identifies the system topology and provides the basis for convergence of each agent's local optimal policy to the global optimal policy. The PM-DQN policy takes advantage of this to manage the system, thereby showing the best expected life-cycle costs at the system level.

[Fig. 11](https://www.sciencedirect.com/science/article/pii/S095183202300426X#fig0011) shows the training history of DCMAC with and without parallel processing. In parallel processing, the optimal learning rates for actor and critical networks are explored in the range of \[10−7,10−3\] and \[10−5,10−3\] respectively, gradually converging to the combination of learning rates with the lowest life-cycle costs. The best learning rates among the explored ones are utilized for single-processing DCMAC as well as the parallelized DCMAC. For the first cycle (i.e., the first 5,000 episodes), there is not much difference between the two results. However, the gap widens considerably right after the synchronization of parallel-trained policies. The difference never diminishes until the end of training, and the superior performance resulting from parallelization is comparable to PM-DQN; comparing them in terms of performance and computation time, PM-DQN is slightly better, but the difference is not significant considering the structural difference between the two models. This implies that the proposed parallel processing framework with periodic synchronization has a dominant effect on performance improvement and it can be transferred to other DRL methods.

![Fig 11](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr11.jpg)

1.  [Download : Download high-res image (227KB)](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr11_lrg.jpg "Download high-res image (227KB)")
2.  [Download : Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S095183202300426X-gr11.jpg "Download full-size image")

Fig. 11. Training history of original and parallelized DCMAC with 95% confidence intervals in Example 2.

## 5\. Summary and conclusions

This paper proposed an optimal decision-making framework based on deep reinforcement learning (DRL), termed parallelized multi-agent deep Q-network (PM-DQN), for efficient risk-informed operation and management (O&M) scheduling of large civil lifeline systems. Existing methods dealt with these system-level O&M scheduling problems by limiting the size of the systems or approximating them as component-level [subproblems](https://www.sciencedirect.com/topics/engineering/subproblem "Learn more about subproblems from ScienceDirect's AI-generated Topic Pages") due to the exponentially growing state and action spaces. However, these types of computational complexity reduction may incur a significant loss of accuracy. Moreover, even if the action space is relaxed, it is still necessary to find the best policy by exploring all policy combinations. To find the optimal policy, the multi-agent credit allocation problem should be solved by inferring the contributions to the centralized cost function. Unstable feedback depending on the policy selection of other agents hinders decentralized policy learning. In contrast, the proposed algorithm overcame this challenge by introducing a divide-and-conquer strategy with community detection and parallel processing. Since network clustering is based on the system topology, the subsystems identified by the Girvan-Newman [clustering algorithm](https://www.sciencedirect.com/topics/engineering/clustering-algorithm "Learn more about clustering algorithm from ScienceDirect's AI-generated Topic Pages") can achieve an appropriate balance between accuracy loss and computational complexity reduction. The strength of the proposed algorithm is further enhanced by reducing the policy exploration time and tuning the optimal hyperparameters in combination with parallel operation. Multiple processing units derive an optimal policy set under hyperparameter tuning and periodic synchronization with the best one. The accuracy and efficiency of PM-DQN were demonstrated on a multi-state general system with 15 components and the California gas distribution system with 48 components. In each numerical example, the optimal PM-DQN policies outperformed baseline alternatives including conventional O&M policies and other MARL-based policies in terms of computational time as well as the expected life-cycle costs. In particular, the California gas distribution system represented as a general system of subsystems through community detection shows the system's topological characteristics prominently, indicating the reason for the good performance of PM-DQN in subsystem level decision-making.

The proposed PM-DQN trains multiple agents simultaneously by defining factorization cost functions based on the causal relationship between the flow capacity losses in the whole system and subsystems. Because the hyperparameter tuning and the entire process of policy exploration are performed independently across multiple processing units, the proposed algorithm has high scalability for the ever-evolving scale of processing units. In addition, the algorithm features flexible handling of the problem, such as adjusting the number of processing units according to the complexity of problems and the computing power. The parallelization remains scalable and flexible even when other MARL algorithms are used. However, there are some other obstacles arising from the limitations of MDPs that hinder the modeling and application of real lifeline network system O&M; it is difficult to consider the time required for actions in the discrete-time domain, and there is a time gap between state change and maintenance actions. In addition, if more diverse action options, such as minor repairs or reinforcements, are given, policy learning at the subsystem level may suffer from the curse of dimensionality. Nevertheless, for efficient MARL and parallel processing, the proposed framework of deploying agents and periodically synchronizing multiple processors can be applied to various other DRL methods. As shown in Example 2, vanilla DRLs specialized for parallel processing are expected to improve exploration and training stability, thereby enabling simple but powerful end-to-end learning without additional clustering or cost splitting.

Furthermore, we can extend the proposed method to a [partially observable MDP](https://www.sciencedirect.com/topics/engineering/partially-observable-markov-decision-process "Learn more about partially observable MDP from ScienceDirect's AI-generated Topic Pages") (POMDP) environment by considering monitoring errors in grasping system states. The computational cost is higher than that in MDP environment because belief function is continuously updated, and monitoring action is additionally considered. However, we can broaden the scalability of the existing algorithms by taking advantage of parallel processing and system simplification based on network clustering. Further research is underway to model state changes in the continuous-time domain and actions that take different times using Semi-Markov decision processes [\[23\]](https://www.sciencedirect.com/science/article/pii/S095183202300426X#bib0023). In addition, the development of such a framework is expected to achieve the scalability to cope with unexpected events (e.g., earthquakes, typhoons).

## CRediT authorship contribution statement

**Dongkyu Lee:** Visualization, Validation, Software, Methodology, Conceptualization, Investigation, Writing – original draft. **Junho Song:** Resources, Project administration, Funding acquisition, Conceptualization, Supervision, Writing – review & editing.
