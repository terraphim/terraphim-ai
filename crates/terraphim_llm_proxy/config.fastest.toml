# Terraphim LLM Proxy - FASTEST Configuration
# Optimized for lowest latency with automatic fallback
#
# Primary: Groq (100+ tokens/sec, ultra-low latency)
# Fallback: DeepSeek, OpenRouter
# Local: Ollama for offline/background tasks
#
# Usage:
#   op run --env-file=.env.fastest -- ./target/release/terraphim-llm-proxy -c config.fastest.toml

[proxy]
host = "127.0.0.1"
port = 3456
api_key = "$PROXY_API_KEY"
timeout_ms = 60000

[router]
# Default: Groq for ultra-low latency (Llama 3.3 70B at 100+ tokens/sec)
default = "groq,llama-3.3-70b-versatile"

# Background: Local Ollama for batch tasks (no network latency)
background = "ollama,qwen2.5-coder:latest"

# Think: DeepSeek Reasoner for complex reasoning (still fast, great quality)
think = "deepseek,deepseek-reasoner"

# Plan Implementation: Groq for speed on tactical tasks
plan_implementation = "groq,llama-3.3-70b-versatile"

# Long context: Gemini Flash for 1M token context at high speed
long_context = "openrouter,google/gemini-2.5-flash-preview-09-2025"
long_context_threshold = 60000

# Web search: Perplexity for real-time web access
web_search = "openrouter,perplexity/llama-3.1-sonar-large-128k-online"

# Image: Claude Sonnet for multimodal (fallback, Groq doesn't support images)
image = "openrouter,anthropic/claude-sonnet-4.5"

# Model mappings for Claude Code compatibility
[[router.model_mappings]]
from = "claude-opus-4-5-*"
to = "groq,llama-3.3-70b-versatile"

[[router.model_mappings]]
from = "claude-sonnet-4-5-*"
to = "groq,llama-3.3-70b-versatile"

[[router.model_mappings]]
from = "claude-haiku-4-5-*"
to = "groq,llama-3.1-8b-instant"

# OpenClaw model mappings
[[router.model_mappings]]
from = "claude-sonnet-4-5"
to = "groq,llama-3.3-70b-versatile"

[[router.model_mappings]]
from = "claude-opus-4-5"
to = "groq,llama-3.3-70b-versatile"

[security.rate_limiting]
enabled = false
requests_per_minute = 600
concurrent_requests = 100

[security.ssrf_protection]
enabled = true
allow_localhost = true
allow_private_ips = true

# Groq - Primary provider (fastest inference)
[[providers]]
name = "groq"
api_base_url = "https://api.groq.com/openai/v1"
api_key = "$GROQ_API_KEY"
models = [
    "llama-3.3-70b-versatile",
    "llama-3.1-8b-instant",
    "mixtral-8x7b-32768"
]
transformers = ["openai"]

# DeepSeek - Secondary provider (fast + reasoning)
[[providers]]
name = "deepseek"
api_base_url = "https://api.deepseek.com"
api_key = "$DEEPSEEK_API_KEY"
models = ["deepseek-chat", "deepseek-reasoner"]
transformers = ["deepseek"]

# OpenRouter - Fallback for specialty models
[[providers]]
name = "openrouter"
api_base_url = "https://openrouter.ai/api/v1"
api_key = "$OPENROUTER_API_KEY"
models = [
    "anthropic/claude-sonnet-4.5",
    "google/gemini-2.5-flash-preview-09-2025",
    "perplexity/llama-3.1-sonar-large-128k-online"
]
transformers = ["openrouter"]

# Ollama - Local fallback (no network, always available)
[[providers]]
name = "ollama"
api_base_url = "http://127.0.0.1:11434"
api_key = "ollama"
models = ["qwen2.5-coder:latest", "llama3.2:latest"]
transformers = ["ollama"]
