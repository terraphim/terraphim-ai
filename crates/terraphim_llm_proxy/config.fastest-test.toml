# Terraphim LLM Proxy - FASTEST Test Configuration
# Tests both Groq and Cerebras for fast inference

[proxy]
host = "127.0.0.1"
port = 3456
api_key = "$PROXY_API_KEY"
timeout_ms = 60000

[router]
# Default: Groq for ultra-low latency
default = "groq,llama-3.3-70b-versatile"

# Background: Cerebras for fast batch tasks
background = "cerebras,llama3.1-8b"

# Think: DeepSeek Reasoner
think = "deepseek,deepseek-reasoner"

[security.rate_limiting]
enabled = false

[security.ssrf_protection]
enabled = true
allow_localhost = true
allow_private_ips = true

# Groq - Primary fast provider
[[providers]]
name = "groq"
api_base_url = "https://api.groq.com/openai/v1"
api_key = "$GROQ_API_KEY"
models = ["llama-3.3-70b-versatile", "llama-3.1-8b-instant"]
transformers = ["openai"]

# Cerebras - Secondary fast provider
[[providers]]
name = "cerebras"
api_base_url = "https://api.cerebras.ai/v1"
api_key = "$CEREBRAS_API_KEY"
models = ["llama3.1-8b", "llama3.1-70b"]
transformers = ["openai"]

# DeepSeek - For think routing
[[providers]]
name = "deepseek"
api_base_url = "https://api.deepseek.com"
api_key = "$DEEPSEEK_API_KEY"
models = ["deepseek-chat", "deepseek-reasoner"]
transformers = ["deepseek"]
