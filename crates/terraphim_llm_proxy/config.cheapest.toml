# Terraphim LLM Proxy - CHEAPEST Configuration
# Optimized for lowest cost with quality fallback
#
# Primary: DeepSeek ($0.14/M input, $0.28/M output - 100x cheaper than GPT-4)
# Background: Ollama (FREE - local inference)
# Fallback: Groq (free tier available)
#
# Estimated costs:
#   - DeepSeek Chat: $0.14/M input tokens, $0.28/M output tokens
#   - DeepSeek Reasoner: $0.55/M input, $2.19/M output
#   - Ollama: FREE (local)
#   - Groq: FREE tier or $0.05/M tokens
#
# Usage:
#   op run --env-file=.env.cheapest -- ./target/release/terraphim-llm-proxy -c config.cheapest.toml

[proxy]
host = "127.0.0.1"
port = 3456
api_key = "$PROXY_API_KEY"
timeout_ms = 120000  # Higher timeout for cost-effective models

[router]
# Default: DeepSeek Chat - excellent quality at 100x lower cost than GPT-4
default = "deepseek,deepseek-chat"

# Background: Local Ollama - completely free, no API costs
background = "ollama,qwen2.5-coder:latest"

# Think: DeepSeek Reasoner - still very cheap for reasoning tasks
think = "deepseek,deepseek-reasoner"

# Plan Implementation: DeepSeek Chat for tactical work
plan_implementation = "deepseek,deepseek-chat"

# Long context: Gemini Flash via OpenRouter (cheap for large context)
long_context = "openrouter,google/gemini-2.5-flash-preview-09-2025"
long_context_threshold = 60000

# Web search: Perplexity (only when needed)
web_search = "openrouter,perplexity/llama-3.1-sonar-large-128k-online"

# Image: Gemini Flash for multimodal (cheaper than Claude)
image = "openrouter,google/gemini-2.5-flash-preview-09-2025"

# Model mappings - route Claude requests to DeepSeek
[[router.model_mappings]]
from = "claude-opus-4-5-*"
to = "deepseek,deepseek-reasoner"

[[router.model_mappings]]
from = "claude-sonnet-4-5-*"
to = "deepseek,deepseek-chat"

[[router.model_mappings]]
from = "claude-haiku-4-5-*"
to = "ollama,qwen2.5-coder:latest"

# OpenClaw model mappings
[[router.model_mappings]]
from = "claude-sonnet-4-5"
to = "deepseek,deepseek-chat"

[[router.model_mappings]]
from = "claude-opus-4-5"
to = "deepseek,deepseek-reasoner"

# GPT model mappings (for Codex CLI)
[[router.model_mappings]]
from = "gpt-4*"
to = "deepseek,deepseek-chat"

[[router.model_mappings]]
from = "gpt-3.5*"
to = "ollama,qwen2.5-coder:latest"

[security.rate_limiting]
enabled = false
requests_per_minute = 600
concurrent_requests = 100

[security.ssrf_protection]
enabled = true
allow_localhost = true
allow_private_ips = true

# DeepSeek - Primary provider (extremely cost-effective)
[[providers]]
name = "deepseek"
api_base_url = "https://api.deepseek.com"
api_key = "$DEEPSEEK_API_KEY"
models = ["deepseek-chat", "deepseek-reasoner"]
transformers = ["deepseek"]

# Ollama - Local provider (completely free)
[[providers]]
name = "ollama"
api_base_url = "http://127.0.0.1:11434"
api_key = "ollama"
models = ["qwen2.5-coder:latest", "llama3.2:latest", "codellama:latest"]
transformers = ["ollama"]

# Groq - Secondary provider (free tier available)
[[providers]]
name = "groq"
api_base_url = "https://api.groq.com/openai/v1"
api_key = "$GROQ_API_KEY"
models = ["llama-3.3-70b-versatile", "llama-3.1-8b-instant"]
transformers = ["openai"]

# OpenRouter - Fallback for specialty models only
[[providers]]
name = "openrouter"
api_base_url = "https://openrouter.ai/api/v1"
api_key = "$OPENROUTER_API_KEY"
models = [
    "google/gemini-2.5-flash-preview-09-2025",
    "perplexity/llama-3.1-sonar-large-128k-online"
]
transformers = ["openrouter"]
