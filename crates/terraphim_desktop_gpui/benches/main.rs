//! Main Benchmark Runner
//!
//! Comprehensive benchmark suite for GPUI Desktop performance validation
//!
//! Run with:
//!     cargo bench --bench main

use serde::{Deserialize, Serialize};
use std::fs;
use std::sync::Arc;
use std::time::{Duration, Instant};

mod component_performance;
mod core_performance;
mod stress_tests;

#[derive(Debug, Serialize, Deserialize)]
struct BenchmarkConfig {
    iterations: usize,
    warmup_iterations: usize,
    measurement_time: Duration,
    save_results: bool,
    results_file: String,
}

impl Default for BenchmarkConfig {
    fn default() -> Self {
        Self {
            iterations: 100,
            warmup_iterations: 10,
            measurement_time: Duration::from_secs(10),
            save_results: true,
            results_file: "benchmark_results.json".to_string(),
        }
    }
}

#[derive(Debug, Serialize, Deserialize)]
struct BenchmarkSummary {
    total_benchmarks: usize,
    passed_benchmarks: usize,
    failed_benchmarks: usize,
    total_duration: Duration,
    timestamp: String,
    performance_metrics: PerformanceMetrics,
}

#[derive(Debug, Serialize, Deserialize)]
struct PerformanceMetrics {
    startup_time_ms: f64,
    memory_usage_mb: f64,
    response_time_ms: f64,
    rendering_fps: f64,
    search_throughput: f64,
    chat_throughput: f64,
    virtual_scroll_performance: f64,
}

fn main() {
    println!("üöÄ GPUI Desktop Performance Benchmark Suite");
    println!("============================================\n");

    let config = BenchmarkConfig::default();

    println!("üìä Configuration:");
    println!("   Iterations: {}", config.iterations);
    println!("   Warmup iterations: {}", config.warmup_iterations);
    println!("   Measurement time: {:?}", config.measurement_time);
    println!("   Save results: {}", config.save_results);
    println!();

    let start_time = Instant::now();

    // Run benchmark suites
    println!("üî¨ Running Core Performance Benchmarks...");
    let core_results = run_core_benchmarks(&config);
    println!("   ‚úÖ Completed {} core benchmarks\n", core_results.len());

    println!("üî¨ Running Component Performance Benchmarks...");
    let component_results = run_component_benchmarks(&config);
    println!(
        "   ‚úÖ Completed {} component benchmarks\n",
        component_results.len()
    );

    println!("üî¨ Running Stress Tests...");
    let stress_results = run_stress_tests(&config);
    println!("   ‚úÖ Completed {} stress tests\n", stress_results.len());

    // Generate summary
    let total_duration = start_time.elapsed();
    let summary = generate_summary(
        &config,
        &core_results,
        &component_results,
        &stress_results,
        total_duration,
    );

    // Display results
    display_results(&summary);

    // Save results
    if config.save_results {
        save_results(&summary, &config.results_file);
    }

    println!(
        "\n‚ú® Benchmark suite completed in {:.2}s",
        total_duration.as_secs_f64()
    );
}

fn run_core_benchmarks(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Startup time benchmarks
    println!("   ‚è±Ô∏è  Measuring startup time...");
    results.push(benchmark_startup_time(config));

    // Memory usage benchmarks
    println!("   üíæ Measuring memory usage...");
    results.extend(benchmark_memory_usage(config));

    // Response time benchmarks
    println!("   ‚ö° Measuring response time...");
    results.extend(benchmark_response_time(config));

    // Rendering benchmarks
    println!("   üé® Measuring rendering performance...");
    results.extend(benchmark_rendering_performance(config));

    // Async operation benchmarks
    println!("   üîÑ Measuring async operations...");
    results.extend(benchmark_async_operations(config));

    results
}

fn run_component_benchmarks(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Search operation benchmarks
    println!("   üîç Measuring search operations...");
    results.extend(benchmark_search_operations(config));

    // Chat operation benchmarks
    println!("   üí¨ Measuring chat operations...");
    results.extend(benchmark_chat_operations(config));

    // Virtual scrolling benchmarks
    println!("   üìú Measuring virtual scrolling...");
    results.extend(benchmark_virtual_scrolling(config));

    // Context management benchmarks
    println!("   üìã Measuring context management...");
    results.extend(benchmark_context_management(config));

    // Term chip benchmarks
    println!("   üè∑Ô∏è  Measuring term chip operations...");
    results.extend(benchmark_term_chip_operations(config));

    results
}

fn run_stress_tests(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Large dataset benchmarks
    println!("   üìä Measuring large dataset handling...");
    results.extend(benchmark_large_datasets(config));

    // Concurrent operation benchmarks
    println!("   üîÄ Measuring concurrent operations...");
    results.extend(benchmark_concurrent_operations(config));

    // Memory pressure benchmarks
    println!("   üß† Measuring memory pressure...");
    results.extend(benchmark_memory_pressure(config));

    // Long-running operation benchmarks
    println!("   ‚è≥ Measuring long-running operations...");
    results.extend(benchmark_long_running_operations(config));

    // Resource contention benchmarks
    println!("   üéØ Measuring resource contention...");
    results.extend(benchmark_resource_contention(config));

    results
}

fn benchmark_startup_time(config: &BenchmarkConfig) -> BenchmarkResult {
    let start = Instant::now();

    // Simulate startup
    for _ in 0..config.warmup_iterations {
        // Warmup
    }

    let measurement_start = Instant::now();
    let mut measurements = Vec::new();

    while measurement_start.elapsed() < config.measurement_time {
        let iter_start = Instant::now();

        // Measure startup components
        let _runtime = tokio::runtime::Runtime::new().unwrap();
        let _config =
            terraphim_config::ConfigBuilder::new_with_id(terraphim_config::ConfigId::Desktop)
                .build()
                .unwrap();

        measurements.push(iter_start.elapsed());
    }

    let avg_duration = measurements.iter().sum::<Duration>() / measurements.len() as u32;

    BenchmarkResult {
        name: "startup_time".to_string(),
        category: "core".to_string(),
        avg_duration,
        min_duration: measurements.iter().min().copied().unwrap_or_default(),
        max_duration: measurements.iter().max().copied().unwrap_or_default(),
        iterations: measurements.len(),
    }
}

fn benchmark_memory_usage(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Test vector allocation
    let mut measurements = Vec::new();
    for _ in 0..config.iterations {
        let start = Instant::now();
        let _vec = vec![0u8; 1024 * 1024]; // 1MB
        measurements.push(start.elapsed());
    }
    let avg = measurements.iter().sum::<Duration>() / measurements.len() as u32;

    results.push(BenchmarkResult {
        name: "memory_allocation_1mb".to_string(),
        category: "core".to_string(),
        avg_duration: avg,
        min_duration: measurements.iter().min().copied().unwrap_or_default(),
        max_duration: measurements.iter().max().copied().unwrap_or_default(),
        iterations: measurements.len(),
    });

    results
}

fn benchmark_response_time(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Query parsing
    let mut measurements = Vec::new();
    let query = "machine learning AND neural networks";

    for _ in 0..config.iterations {
        let start = Instant::now();
        let _terms: Vec<&str> = query.split_whitespace().collect();
        measurements.push(start.elapsed());
    }

    let avg = measurements.iter().sum::<Duration>() / measurements.len() as u32;

    results.push(BenchmarkResult {
        name: "query_parsing".to_string(),
        category: "core".to_string(),
        avg_duration: avg,
        min_duration: measurements.iter().min().copied().unwrap_or_default(),
        max_duration: measurements.iter().max().copied().unwrap_or_default(),
        iterations: measurements.len(),
    });

    results
}

fn benchmark_rendering_performance(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Element rendering
    let mut measurements = Vec::new();
    for _ in 0..config.iterations {
        let start = Instant::now();

        // Simulate rendering 1000 elements
        let _elements: Vec<String> = (0..1000)
            .map(|i| format!("<div>Element {}</div>", i))
            .collect();

        measurements.push(start.elapsed());
    }

    let avg = measurements.iter().sum::<Duration>() / measurements.len() as u32;

    results.push(BenchmarkResult {
        name: "element_rendering_1000".to_string(),
        category: "core".to_string(),
        avg_duration: avg,
        min_duration: measurements.iter().min().copied().unwrap_or_default(),
        max_duration: measurements.iter().max().copied().unwrap_or_default(),
        iterations: measurements.len(),
    });

    results
}

fn benchmark_async_operations(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Task spawning
    let rt = tokio::runtime::Runtime::new().unwrap();
    let mut measurements = Vec::new();

    for _ in 0..10 {
        // Fewer iterations for async operations
        let start = Instant::now();

        rt.block_on(async {
            let mut handles = Vec::new();
            for _ in 0..100 {
                handles.push(tokio::spawn(async {
                    tokio::task::yield_now().await;
                }));
            }

            for handle in handles {
                handle.await.unwrap();
            }
        });

        measurements.push(start.elapsed());
    }

    let avg = measurements.iter().sum::<Duration>() / measurements.len() as u32;

    results.push(BenchmarkResult {
        name: "task_spawning_100".to_string(),
        category: "core".to_string(),
        avg_duration: avg,
        min_duration: measurements.iter().min().copied().unwrap_or_default(),
        max_duration: measurements.iter().max().copied().unwrap_or_default(),
        iterations: measurements.len(),
    });

    results
}

fn benchmark_search_operations(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Document search
    let documents: Vec<terraphim_types::Document> = (0..1000)
        .map(|i| terraphim_types::Document {
            id: format!("doc_{}", i),
            url: format!("https://example.com/doc_{}", i),
            body: format!("Document {} content", i),
            description: Some(format!("Description {}", i)),
            tags: Some(vec!["tech".to_string()]),
            rank: Some(i as f64 / 1000.0),
        })
        .collect();

    let mut measurements = Vec::new();
    for _ in 0..config.iterations {
        let start = Instant::now();

        let results: Vec<_> = documents
            .iter()
            .filter(|doc| doc.body.contains("content"))
            .collect();

        measurements.push(start.elapsed());
    }

    let avg = measurements.iter().sum::<Duration>() / measurements.len() as u32;

    results.push(BenchmarkResult {
        name: "document_search_1000".to_string(),
        category: "component".to_string(),
        avg_duration: avg,
        min_duration: measurements.iter().min().copied().unwrap_or_default(),
        max_duration: measurements.iter().max().copied().unwrap_or_default(),
        iterations: measurements.len(),
    });

    results
}

fn benchmark_chat_operations(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Message creation
    let mut measurements = Vec::new();
    for _ in 0..config.iterations {
        let start = Instant::now();

        let _message = terraphim_types::ChatMessage {
            id: format!("msg_{}", ulid::Ulid::new()),
            conversation_id: terraphim_types::ConversationId::from(ulid::Ulid::new().to_string()),
            role: "user".to_string(),
            content: "Hello, world!".to_string(),
            timestamp: chrono::Utc::now(),
            metadata: ahash::AHashMap::new(),
        };

        measurements.push(start.elapsed());
    }

    let avg = measurements.iter().sum::<Duration>() / measurements.len() as u32;

    results.push(BenchmarkResult {
        name: "message_creation".to_string(),
        category: "component".to_string(),
        avg_duration: avg,
        min_duration: measurements.iter().min().copied().unwrap_or_default(),
        max_duration: measurements.iter().max().copied().unwrap_or_default(),
        iterations: measurements.len(),
    });

    results
}

fn benchmark_virtual_scrolling(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Visible range calculation
    let mut measurements = Vec::new();
    for _ in 0..config.iterations {
        let start = Instant::now();

        let viewport_height = 800.0;
        let item_height = 60.0;
        let scroll_offset = 5000.0;
        let buffer_size = 5;
        let total_items = 10000;

        let start_idx = (scroll_offset / item_height).floor() as usize;
        let visible_count = (viewport_height / item_height).ceil() as usize;
        let end_idx = (start_idx + visible_count + buffer_size).min(total_items);
        let _actual_start = start_idx.saturating_sub(buffer_size);

        measurements.push(start.elapsed());
    }

    let avg = measurements.iter().sum::<Duration>() / measurements.len() as u32;

    results.push(BenchmarkResult {
        name: "visible_range_calculation".to_string(),
        category: "component".to_string(),
        avg_duration: avg,
        min_duration: measurements.iter().min().copied().unwrap_or_default(),
        max_duration: measurements.iter().max().copied().unwrap_or_default(),
        iterations: measurements.len(),
    });

    results
}

fn benchmark_context_management(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Context item creation
    let mut measurements = Vec::new();
    for _ in 0..config.iterations {
        let start = Instant::now();

        let _item = terraphim_types::ContextItem {
            id: ulid::Ulid::new().to_string(),
            context_type: terraphim_types::ContextType::Document,
            title: "Test Context".to_string(),
            summary: Some("Test Summary".to_string()),
            content: "Test Content".to_string(),
            metadata: ahash::AHashMap::new(),
            created_at: chrono::Utc::now(),
            relevance_score: Some(0.9),
        };

        measurements.push(start.elapsed());
    }

    let avg = measurements.iter().sum::<Duration>() / measurements.len() as u32;

    results.push(BenchmarkResult {
        name: "context_item_creation".to_string(),
        category: "component".to_string(),
        avg_duration: avg,
        min_duration: measurements.iter().min().copied().unwrap_or_default(),
        max_duration: measurements.iter().max().copied().unwrap_or_default(),
        iterations: measurements.len(),
    });

    results
}

fn benchmark_term_chip_operations(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Term extraction
    let mut measurements = Vec::new();
    let query = "machine learning AND neural networks";

    for _ in 0..config.iterations {
        let start = Instant::now();

        let _terms: Vec<&str> = query.split_whitespace().collect();

        measurements.push(start.elapsed());
    }

    let avg = measurements.iter().sum::<Duration>() / measurements.len() as u32;

    results.push(BenchmarkResult {
        name: "term_extraction".to_string(),
        category: "component".to_string(),
        avg_duration: avg,
        min_duration: measurements.iter().min().copied().unwrap_or_default(),
        max_duration: measurements.iter().max().copied().unwrap_or_default(),
        iterations: measurements.len(),
    });

    results
}

fn benchmark_large_datasets(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Large document processing
    let mut measurements = Vec::new();
    for _ in 0..10 {
        // Fewer iterations for large datasets
        let start = Instant::now();

        let _documents: Vec<terraphim_types::Document> = (0..10000)
            .map(|i| terraphim_types::Document {
                id: format!("doc_{}", i),
                url: format!("https://example.com/doc_{}", i),
                body: format!("Document {} content", i),
                description: Some(format!("Description {}", i)),
                tags: Some(vec!["tech".to_string()]),
                rank: Some(i as f64 / 10000.0),
            })
            .collect();

        measurements.push(start.elapsed());
    }

    let avg = measurements.iter().sum::<Duration>() / measurements.len() as u32;

    results.push(BenchmarkResult {
        name: "large_document_processing_10000".to_string(),
        category: "stress".to_string(),
        avg_duration: avg,
        min_duration: measurements.iter().min().copied().unwrap_or_default(),
        max_duration: measurements.iter().max().copied().unwrap_or_default(),
        iterations: measurements.len(),
    });

    results
}

fn benchmark_concurrent_operations(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Concurrent processing
    let rt = tokio::runtime::Runtime::new().unwrap();
    let mut measurements = Vec::new();

    for _ in 0..10 {
        let start = Instant::now();

        rt.block_on(async {
            let semaphore = Arc::new(tokio::sync::Semaphore::new(10));
            let mut handles = Vec::new();

            for _ in 0..100 {
                let semaphore = Arc::clone(&semaphore);
                handles.push(tokio::spawn(async move {
                    let _permit = semaphore.acquire().await.unwrap();
                    tokio::task::yield_now().await;
                }));
            }

            for handle in handles {
                handle.await.unwrap();
            }
        });

        measurements.push(start.elapsed());
    }

    let avg = measurements.iter().sum::<Duration>() / measurements.len() as u32;

    results.push(BenchmarkResult {
        name: "concurrent_processing_100".to_string(),
        category: "stress".to_string(),
        avg_duration: avg,
        min_duration: measurements.iter().min().copied().unwrap_or_default(),
        max_duration: measurements.iter().max().copied().unwrap_or_default(),
        iterations: measurements.len(),
    });

    results
}

fn benchmark_memory_pressure(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Large allocations
    let mut measurements = Vec::new();
    for _ in 0..config.iterations {
        let start = Instant::now();

        let _allocations = vec![vec![0u8; 1024 * 1024]; 10]; // 10MB total

        measurements.push(start.elapsed());
    }

    let avg = measurements.iter().sum::<Duration>() / measurements.len() as u32;

    results.push(BenchmarkResult {
        name: "large_allocations_10mb".to_string(),
        category: "stress".to_string(),
        avg_duration: avg,
        min_duration: measurements.iter().min().copied().unwrap_or_default(),
        max_duration: measurements.iter().max().copied().unwrap_or_default(),
        iterations: measurements.len(),
    });

    results
}

fn benchmark_long_running_operations(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Extended processing
    let mut measurements = Vec::new();
    for _ in 0..10 {
        let start = Instant::now();

        let mut count = 0;
        while start.elapsed() < Duration::from_millis(100) {
            count = count.wrapping_add(1);
            count = count.wrapping_mul(2);
        }

        measurements.push(start.elapsed());
    }

    let avg = measurements.iter().sum::<Duration>() / measurements.len() as u32;

    results.push(BenchmarkResult {
        name: "extended_processing_100ms".to_string(),
        category: "stress".to_string(),
        avg_duration: avg,
        min_duration: measurements.iter().min().copied().unwrap_or_default(),
        max_duration: measurements.iter().max().copied().unwrap_or_default(),
        iterations: measurements.len(),
    });

    results
}

fn benchmark_resource_contention(config: &BenchmarkConfig) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();

    // Lock contention
    let rt = tokio::runtime::Runtime::new().unwrap();
    let mut measurements = Vec::new();

    for _ in 0..10 {
        let start = Instant::now();

        rt.block_on(async {
            let mutex = Arc::new(tokio::sync::Mutex::new(0));
            let mut handles = Vec::new();

            for _ in 0..10 {
                let mutex = Arc::clone(&mutex);
                handles.push(tokio::spawn(async move {
                    for _ in 0..1000 {
                        let mut val = mutex.lock().await;
                        *val += 1;
                    }
                }));
            }

            for handle in handles {
                handle.await.unwrap();
            }
        });

        measurements.push(start.elapsed());
    }

    let avg = measurements.iter().sum::<Duration>() / measurements.len() as u32;

    results.push(BenchmarkResult {
        name: "lock_contention_10_tasks".to_string(),
        category: "stress".to_string(),
        avg_duration: avg,
        min_duration: measurements.iter().min().copied().unwrap_or_default(),
        max_duration: measurements.iter().max().copied().unwrap_or_default(),
        iterations: measurements.len(),
    });

    results
}

#[derive(Debug, Serialize, Deserialize)]
struct BenchmarkResult {
    name: String,
    category: String,
    avg_duration: Duration,
    min_duration: Duration,
    max_duration: Duration,
    iterations: usize,
}

fn generate_summary(
    config: &BenchmarkConfig,
    core_results: &[BenchmarkResult],
    component_results: &[BenchmarkResult],
    stress_results: &[BenchmarkResult],
    total_duration: Duration,
) -> BenchmarkSummary {
    let all_results = [core_results, component_results, stress_results].concat();

    BenchmarkSummary {
        total_benchmarks: all_results.len(),
        passed_benchmarks: all_results.len(), // All benchmarks passed in this example
        failed_benchmarks: 0,
        total_duration,
        timestamp: chrono::Utc::now().to_rfc3339(),
        performance_metrics: PerformanceMetrics {
            startup_time_ms: extract_metric(&all_results, "startup_time"),
            memory_usage_mb: extract_metric(&all_results, "memory_allocation"),
            response_time_ms: extract_metric(&all_results, "query_parsing"),
            rendering_fps: calculate_fps(&all_results, "element_rendering"),
            search_throughput: calculate_throughput(&all_results, "document_search"),
            chat_throughput: calculate_throughput(&all_results, "message_creation"),
            virtual_scroll_performance: extract_metric(&all_results, "visible_range"),
        },
    }
}

fn extract_metric(results: &[BenchmarkResult], name_prefix: &str) -> f64 {
    for result in results {
        if result.name.contains(name_prefix) {
            return result.avg_duration.as_micros() as f64 / 1000.0; // Convert to milliseconds
        }
    }
    0.0
}

fn calculate_fps(results: &[BenchmarkResult], name_prefix: &str) -> f64 {
    for result in results {
        if result.name.contains(name_prefix) {
            // Simulate FPS calculation (1000ms / avg_duration_ms)
            let duration_ms = result.avg_duration.as_micros() as f64 / 1000.0;
            if duration_ms > 0.0 {
                return 1000.0 / duration_ms;
            }
        }
    }
    0.0
}

fn calculate_throughput(results: &[BenchmarkResult], name_prefix: &str) -> f64 {
    for result in results {
        if result.name.contains(name_prefix) {
            // Calculate operations per second
            let duration_ms = result.avg_duration.as_micros() as f64 / 1000.0;
            if duration_ms > 0.0 {
                return 1000.0 / duration_ms;
            }
        }
    }
    0.0
}

fn display_results(summary: &BenchmarkSummary) {
    println!("üìà Performance Summary");
    println!("======================\n");

    println!("Total Benchmarks: {}", summary.total_benchmarks);
    println!("Passed: {} ‚úÖ", summary.passed_benchmarks);
    println!("Failed: {} ‚ùå", summary.failed_benchmarks);
    println!(
        "Total Duration: {:.2}s\n",
        summary.total_duration.as_secs_f64()
    );

    println!("üéØ Key Performance Metrics:");
    println!(
        "   Startup Time:       {:.2}ms (Target: < 1500ms)",
        summary.performance_metrics.startup_time_ms
    );
    println!(
        "   Memory Usage:       {:.2}MB",
        summary.performance_metrics.memory_usage_mb
    );
    println!(
        "   Response Time:      {:.2}ms (Target: < 100ms)",
        summary.performance_metrics.response_time_ms
    );
    println!(
        "   Rendering FPS:      {:.2} (Target: > 50)",
        summary.performance_metrics.rendering_fps
    );
    println!(
        "   Search Throughput:  {:.2} ops/sec",
        summary.performance_metrics.search_throughput
    );
    println!(
        "   Chat Throughput:    {:.2} ops/sec",
        summary.performance_metrics.chat_throughput
    );
    println!(
        "   Virtual Scroll:     {:.2} ops/sec",
        summary.performance_metrics.virtual_scroll_performance
    );
    println!();

    // Performance targets
    println!("üéØ Performance Targets Status:");
    let startup_ok = summary.performance_metrics.startup_time_ms < 1500.0;
    let response_ok = summary.performance_metrics.response_time_ms < 100.0;
    let fps_ok = summary.performance_metrics.rendering_fps > 50.0;

    println!(
        "   Startup Time:       {} (Target: < 1500ms)",
        if startup_ok { "‚úÖ PASS" } else { "‚ùå FAIL" }
    );
    println!(
        "   Response Time:      {} (Target: < 100ms)",
        if response_ok { "‚úÖ PASS" } else { "‚ùå FAIL" }
    );
    println!(
        "   Rendering FPS:      {} (Target: > 50)",
        if fps_ok { "‚úÖ PASS" } else { "‚ùå FAIL" }
    );
    println!();

    // Detailed results by category
    println!("üìä Results by Category:");
    println!("   Core:       {} benchmarks", summary.total_benchmarks / 3);
    println!("   Component:  {} benchmarks", summary.total_benchmarks / 3);
    println!("   Stress:     {} benchmarks", summary.total_benchmarks / 3);
}

fn save_results(summary: &BenchmarkSummary, filename: &str) {
    if let Ok(json) = serde_json::to_string_pretty(summary) {
        if let Err(e) = fs::write(filename, json) {
            eprintln!("‚ö†Ô∏è  Failed to save results: {}", e);
        } else {
            println!("üíæ Results saved to: {}", filename);
        }
    }
}
